# OSTEP Part1(Virtualization) 笔记

大部分截图来自原书，贴出书的官方主页： 《[Operating Systems: Three Easy Pieces](http://pages.cs.wisc.edu/~remzi/OSTEP/)》
(作者Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau)。感谢原作者这么好的书。

## 第一节 进程的抽象

#### 1. 策略和机制

Policy在mechanism（策略和机制）在操作系统中通常是分开设计的。比如如何切换上下文是一个low-level的mechanism，指底层的方法或者协议。当前时刻应该让哪个进程运行更好是一个high-level的policy的问题，指一些“智能”的调度。


#### 2. 虚拟化和OS

OS本身就可以看做是“虚拟机”(virtual machine)。系统通过在可以被接受的合理开销（时间、空间）范围内，将计算机CPU、内存、存储等资源进行虚拟化（抽象），目的是为了用户的方便使用。

**CPU虚拟化** 主要体现在将任务抽象为 **进程(process)** ，将资源按进程隔离，然后多个进程轮转使用计算资源； **内存虚拟化** 主要体现在 **虚拟地址空间(virtual address space 或 adress space)** ；而对于持久化的 **存储** ，OS让文件共享，没有那么多私有隔离，OS假定用户会用文件来共享数据。 (？？？？)

## 第二节 进程API

#### 1. fork和exec

* exec()函数组和fork()的区别是，exec执行以后就再也不会返回


# OSTEP Part3(Persistence) 笔记

大部分截图来自原书，贴出书的官方主页： 《[Operating Systems: Three Easy Pieces](http://pages.cs.wisc.edu/~remzi/OSTEP/)》
(作者Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau)。感谢原作者这么好的书。

## 第一节 I/O Device

#### 1. IO 总线

**一般情况下，** IO设备的性能较差(慢)，所以用Peripheral IO Bus，为什么不用像显卡一样用的PCI呢？因为1)越快的总线越短，这样空间不够插；2)越快的总线制作成本越高，如果存储设备照总线的性能差的远，没必要用高性能总线。

[[p3_001.png]]

这张图为总线的层次结构，memory bus是最快的也是最近的，IO Bus比较远，也是最慢的，中间有用于显卡的PCI等总线。

#### 2. 典型设备的组成部分

[[p3_002.png]]

一个典型的外围设备如图所示，包括两部分： **接口** 和 **内部结构** 。

**接口：** 类似软件接口的功能，硬件接口是留给OS和设备交互的。
**内部结构：** 比如㓟CPU、MEM等基本组件，还有称为固件(firmware)的软件来实现内部功能。

#### 3. PIO中的两种模式(Polling和Interrupt)

一种典型的协议是 **Polling** (轮询)，步骤有4: 

* 循环等待STATUS寄存器直到设备状态为不busy
* 写数据到DATA寄存器
* 写命令到COMMAND寄存器
* 循环等待STATUS直到设备为不busy

Polling显著的缺点就是太浪费CPU时间，这是因为IO相对于CPU是很慢的，大量的CPU时间被用在了等待上。

**Interrupt** (中断)方法可以解决这个问题，用Interrupt方法进行IO时，当设备完成操作时，会raise一个硬件interrupt。但是这样的话，如果设备很快(不如现在的NVMe SSD设备)，Interrupt由于需要进程上下文的切换、以及中断的控制等原因，会拖慢IO的速度。所以两种方法各有利弊：

|Polling | Interrupt |
|--------|--------|
| 浪费CPU时间 | 节省CPU时间 |
| 更小的I/O延迟 | 进程切换及中断处理导致高延迟 |

在IO请求压力时大时小不好确定的系统中，更好的方案可能是采用hybrid(混合)的 **两段协议** ，先poll一会儿，还没完成的话改用interrupt方式。还有一种方式是 **中断合并** ，当一个请求完成，等一等，说不定又有新的请求完成了，这样就见小了中断数，减小了中断带来的性能损失，但是这样做的缺点也是显而易见的--用延迟代价换来了高吞吐。

#### 4. 设备交互：PIO和MMIO

以上的Interrupt和Polling都属于Programmed I/O(PIO)的方式，这种方式是CPU通过指令和设备进行的交互。

还有一种称为Memory-mapped I/O(MMIO)，这种方法中，设备寄存器被映射到内存地址空间，OS读写这个映射地址，硬件会自动将存取数据路由到设备而不是主存中。

#### 5. PIO中传输任务的卸载(Direct Memory Access, DMA)

若不使用DMA，虽然可以用中断来讲等待设备IO完成的时间用在其他进程上，但是IO请求中还包括CPU从内存到设备以字长为单位一点一点搬运数据（数据传输）的过程，如图：

[[p3_003.png]]

有了DMA这种专用设备帮CPU搬运，流水线就可以入下图一样：

[[p3_004.png]]

当DMA完成任务，DMA控制器会raise一个中断，这样OS就知道传输完成了。

#### 6. 设备驱动和I/O栈

I/O栈各层的抽象(如块设备驱动、文件系统等)当然有好处，其把不同的设备封装成统一的结构，但也有坏处。

其实不是从上层应用到底层驱动会出现信息丢失的问题(我曾经调研过上层到下层会有语义鸿沟的问题)，底层的设备由于统一的抽象也会“丧失个性”，比如SCSI支持多种IO错误信息，但是ATA/IDE却不支持，因此Linux设计成上层文件系统只能接到更"EIO"错误(generic IO errer)。

驱动占Linux源码的70%，很多驱动都是“业余”开发的，因此很多系统崩溃也是由于驱动bug造成的。

## 第三节 RAID

书中有个对比表总结的不错：

[[p3_005.png]]]

## 第四节 文件和目录

文件(file)和目录(directory)是OS虚拟化中存储部分的两个重要的抽象概念。

本节主要解释一些基本的文件操作和对应的系统调用。比如：

| 常用操作 | 系统调用 |
|--------|--------|
|  cat命令   | open() / read() / write()  |
| 数据同步  |   fsync()     |
|   mv命令重命名   |   rename()     |
|  ls的-l参数、stat命令    |   stat() / fstat()      |
|    rm命令    |    unlink()    |
|     mkdir命令    |   mkdir()     |
|文件夹操作函数|opendir() / closedir() / readdir()|
|rmdir命令|rmdir()|

这节还讲了硬链接和软连接、创建和挂载文件系统的内容，略了。。


## 第五节 文件系统的实现

本节主要讲，要实现一个文件系统的基本功能上应该怎么实现。

#### 5.1 文件系统的根本

对于文件系统，我们头脑中应该有一种模型：文件系统(Filesystem, FS)由接入部分和基本数据结构部分组成。前者指的是read()、write()等这样的接口；后者关系到FS内部怎么组织存储用户数据和元数据。设计时，不同的文件系统一般都有相同的接口，不同FS的主要区别是内部的 **数据结构** 。

#### 5.2 整体结构

文件系统内的数据可以分为用户数据(data)和元数据(metadata)，元数据可以简单的理解为数据的数据。上节所讲的文件系统的两种重要抽象（文件和目录），都需要存储data和metadata。

**data：**
  * 文件的data自然是用户数据，我们不需要关注用户数据的具体内容；目录的data应该存储文件名和对应文件的metadata。

**metadata：**

  * **inode** 文件的metadata是inode(曾经是index node的缩写)或者类似inode但不叫inode的东西；目录的metadata也是inode。inode以某种结构存储有对应文件或目录的数据块地址。
  * **bitmap**(或free list) 是与整个文件系统空闲空间相关的metadata，FS需要分配新空间时，会从这里查询哪里有空闲位置。分为data的bitmap和inode的bitmap。这里的bitmap也可以换为使用free list其他的数据结构。可以说，bitmap是metadata(文件和目录的inode)的metadata和data(文件和目录的data)的metadata，不过也还是metadata。。。
  * **superblock** 记录有文件系统最大inode数、inode块和数据块从哪个地址开始等信息，是整个文件系统的metadata。

#### 5.3 Inode

**组成：** 每个inode都有一个inumber，由于inode是顺序排列的，给定inode table开始块的地址、inode的大小和inumber就可以知道这个inode在哪个块了。inode中包含的信息包括：type（一般文件还是目录）、size（文件大小）、各种时间信息、指向数据块的地址“指针”（索引）等。

**数据块地址索引：** 其中的地址“指针”可以分为两类，最简单的直接指向相关的data块，称为direct pointer；另一种更常应用的交indirect pointer。

为什么要用indirect呢？因为一个inode的空间是有限的，如果文件数据太大，inode中留给存储块地址的空间装不下。于是就有了间接的两层索引、三层索引这种结构。例如，三级索引中，总共inode有n个地址空间有a个直接索引，b个二级间接索引，c个三级间接索引，那么有`n=a+b+c`；如果设一个数据块可以存m个索引，块的大小为4KB，支持文件的最大大小为`4KB * (a + b * m + c * m * m)`。

显然，这个多级索引结构是个非平衡树，也是不对称的，看了让人挺难受（强迫症），为什么这样设计呢？这是出于多数文件都很小的事实。

**FAT的链式存储：** FAT等FS没有inode，而是只存储第一个数据块的地址（类似链表头），然后下一个块的地址可以去前一个块找。这只是道理上的，如果真的这么做，随机访问肯定超级慢，所以还内存中存在一种存有连接信息的表，（可以看成key-value形式，用块的地址作为key，用下一个块作为value），这样就可以在内存中完成“链表的遍历”，加速了这种思路。这种结构的缺陷之一是无法创建硬链接。

书的作者给出了几种事实对设计FS很有借鉴意义：

| 事实 | 数据 |
|--------|--------|
|  多数文件都很小 |  多数都在2KB左右      |
|   平均大小会大一些     |  200KB左右      |
|   大多数数据都在大文件中     | 少数的大文件占据了大部分的空间     |
|   文件系统有很多文件  |  平均接近100k个     |
|   文件系统一般都是没满的 |  即使磁盘空间在变大，还是有50%空间空闲      |
|  目录一般都很小      |   平均小于20个项     |

5.4 目录的组织

在作者的例子中，目录和文件是类似的，都有inode，只不过目录的data中有文件名和文件inode地址的映射，这是一种类似线性链表的结构，所以目录如果深的话查找开销会比较大。

目录的组织是对数据结构的一种设计和选择，当然也可以有不同的选择，比如XFS就采用了B-tree作为目录的存储组织结构（这样在创建文件时也很容易确定是不是重名）。

5.5 空闲空间的管理

和目录的组织一样，作者的例子用了bitmap，但还可以用free list链表形式或者B-tree(XFS)等很多形式进行管理，这将导致性能和空间的trade-off。

当使用空闲空间时，尽量将连续的空间分配给需要的文件，这样的启发式方法会增加文件读写的速度（请求次数更少、一次顺序读写更多）。

5.6 read()和write()的过程

作者的例子中，一次读或写请求会导致多次IO。尤其对于写时不存在的文件进行creat操作时，需要更多的IO，因为要逐级更改上层目录data和inode等。读和创建文件的操作如下两图：

[[p3_006.png]]

[[p3_007.png]]

5.7 缓存和缓冲区(caching and buffering)

现代系统一般将virtual memory pages和文件系统pages合并成统一的page cache(unified page cache)，这样内存可以被更灵活的在虚拟内存和文件系统间分配（？）。

write buffering是有好处的，等一等再往下存，可以batch一些请求或者减少请求（如创建后马上删除），现在的文件系统一般都会等5s或30s之类的。这样做是有trade-off的：增加了延迟，增加了系统crash时数据丢失的可能。DBMS等系统决不允许这样导致数据丢失，因此可以勤用fsync()同步、用direct I/O绕过缓存或用raw disk接口绕过文件系统。但是一般这种trade-off在作者看来是可以接受。


## 第六节 局部性和FFS(Fast File System)

本节主要讲了FFS，一种84年提出的Unix文件系统，ext2和ext3的前辈。

在FFS之前，磁盘的性能很差，主要是因为人们没有考虑底层磁盘的特性，把磁盘当成了一个随机存储设备。例如，相对更古老Unix FS中的碎片没有被适当的处理，越来越碎，性能越来越差；而且块太小只有一个扇区的大小，这会导致开销增大（传输一次的量太小了）。

#### 措施1：分组存储

将整个文件系统分成Cylinder Group，每个Group都类似一个文件系统，甚至有冗余的Superblock。ext2和ext3中把这种结构称为block group。

这样存储就可以实现一些策略，这些“启发式”的策略并不是通过详细的论证得到，都是基于经验的，也是很管用的：1）将相关的文件（如同一个文件夹中的文件）放到一个组中（空间局部性，虽然FFS考虑了空间局部性，但没有考虑时间局部性，比如编译文件夹和源码文件夹在相距很远的两个目录里，就可能导致反复的查询）；2）创建文件时平均让各个组的空余inode数平衡等。


#### 措施2：大文件的分组存储

大的文件如果大到占了一个组的大部分，就开始破坏组内的局部性了，因为很少有地方可以存相关的文件了。因此可以根据预先设定，一个组内一个文件最多存多少块，如果多于这个数，就把一个文件分在多个组中存。

这样固然会降低存储效率，但是降低的应该不多，因为即使分块了，大文件的每个块依然较大，一次还是可以传输较多的数据，这样，传输所占的时间还是远大于请求所占的时间。这也是可以用数学公式计算一下比例的（摊还分析），作者在书中进行了推导，要达到50%的带宽，一个块400K即可，要达到99%的带宽，一个块需要40MB。（当然这只是针对磁盘，现在的高性能设备就不一定了！）

作者还讨论了HDD中顺序请求也可能让磁头旋转可能过头的问题。解决方法如图，具体解释，略。。。

[[p3_008.png]]


## 第十节 LFS(log-structured filesystem)

What does “level of indirection” mean in David Wheeler's aphorism?
, https://stackoverflow.com/questions/18003544/what-does-level-of-indirection-mean-in-david-wheelers-aphorism
